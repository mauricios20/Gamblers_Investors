{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "import counting_fns as cf\n",
    "\n",
    "\n",
    "# List of all the months\n",
    "months = ['2_June', '3_July', '4_August', '5_September', '6_October']\n",
    "\n",
    "# Define the path to the data directory and columns to keep\n",
    "data_path = \"/Users/mau/Library/CloudStorage/Dropbox/Mac/Documents/Dissertation/Chapter 2/Entire_Data/By month\"\n",
    "\n",
    "# Filter Columns\n",
    "filter = ['session_time', 'gender', 'age_gen', 'day', 'timeofday', 'first_outcome',\n",
    "        'first_wager','first_p/b', 'last_outcome', 'last_wager', 'last_p/b',\n",
    "        'beginning_amt', 'ending_amt', 'ending_balance', 'ave_slotdenom', \n",
    "        'std_slotdenom', 'min_slotdenom', 'max_slotdenom', 'ave_theo_payback',\n",
    "        'min_theo_payback', 'max_theo_payback', 'ave_wageramt', 'std_wageramt',\n",
    "        'min_wager', 'max_wager', 'ave_p/b', 'std_p/b', 'max_p/b', 'max_profit', 'depletion_slope', \n",
    "        '#inc_slotdenom', '#dec_slotdenom', '#inc_maxbet', '#dec_maxbet', '#W', '#L', '#NH', '#D',\n",
    "        'w/min', 'l/min', '#2ws', '2ws_profit', '2ws_wgramt','2ws/min', \n",
    "        '#3ws', '3ws_profit', '3ws_wgramt', '3ws/min', '#4ws', '4ws_profit', '4ws_wgramt', '4ws/min', \n",
    "        'w/g', 'l/g', 'nh/g', 'd/g', 'ave_time_per_gamble', \n",
    "        'min_time_per_gamble', 'max_time_per_gamble', 'total_gambles',\n",
    "        'machines_changes', 'unique_machines', 'ave_time_per_machine', 'classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Months in training set: ['6_October', '5_September', '3_July', '2_June']\n",
      "Months in test set: ['4_August']\n"
     ]
    }
   ],
   "source": [
    "# Randomly select 3 months for training set\n",
    "# random.seed(350)\n",
    "months_train = random.sample(months, 4)\n",
    "\n",
    "# Print the months in the training set\n",
    "print(\"Months in training set:\", months_train)\n",
    "\n",
    "# Create a list of remaining months for the test set\n",
    "months_test = [month for month in months if month not in months_train]\n",
    "# Print the months in the test set\n",
    "print(\"Months in test set:\", months_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10408    11]\n",
      " [ 1168     8]]\n",
      "Accuracy:  0.8983182406209573\n",
      "Precision:  0.6600771105372276\n",
      "Recall:  0.5028734787897307\n",
      "F1 Score:  0.4798930148635911\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "datasets = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '1min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder__x23: 0.006226066739606084\n",
      "remainder__x14: 0.004998632385120316\n",
      "remainder__x13: 0.002523249452954013\n",
      "remainder__x24: 0.0021710886214441837\n",
      "remainder__x21: 0.0015522428884025997\n",
      "remainder__x22: 0.001442833698030621\n",
      "remainder__x6: 0.001179567833697992\n",
      "remainder__x3: 0.0009949398249452734\n",
      "remainder__x11: 0.0009539113785557718\n",
      "remainder__x12: 0.00033164660831508375\n",
      "remainder__x55: 0.00021539934354484203\n",
      "remainder__x7: 0.00017437089715532927\n",
      "encoder__x1_Silent: 0.00016753282275708826\n",
      "remainder__x53: 0.0001641137855579511\n",
      "encoder__x1_Baby Boomers: 0.00010940919037196739\n",
      "remainder__x16: 0.00010257111597371527\n",
      "remainder__x57: 7.521881838074007e-05\n",
      "remainder__x48: 7.179978118163622e-05\n",
      "remainder__x25: 7.17997811816251e-05\n",
      "remainder__x19: 6.838074398248794e-05\n",
      "remainder__x40: 5.470459518598369e-05\n",
      "remainder__x51: 4.786652078772047e-05\n",
      "remainder__x47: 4.4447483588638816e-05\n",
      "remainder__x30: 4.102844638951275e-05\n",
      "remainder__x38: 4.102844638950165e-05\n",
      "remainder__x33: 4.1028446389490544e-05\n",
      "remainder__x54: 4.1028446389479445e-05\n",
      "remainder__x52: 3.760940919033118e-05\n",
      "remainder__x43: 3.4190371991260626e-05\n",
      "remainder__x59: 3.4190371991260626e-05\n",
      "remainder__x37: 2.3933260393882437e-05\n",
      "remainder__x50: 1.7095185995630313e-05\n",
      "remainder__x31: 1.3676148796504251e-05\n",
      "remainder__x27: 1.3676148796493148e-05\n",
      "remainder__x35: 1.3676148796482045e-05\n",
      "encoder__x2_loss: 1.0257111597378188e-05\n",
      "remainder__x45: 1.0257111597378188e-05\n",
      "remainder__x28: 6.838074398252126e-06\n",
      "remainder__x4: 3.419037199126063e-06\n",
      "remainder__x42: 3.419037199126063e-06\n",
      "encoder__x2_near-hit: 0.0\n",
      "encoder__x5_draw: 0.0\n",
      "remainder__x29: 0.0\n",
      "remainder__x46: 0.0\n",
      "encoder__x5_gain: -3.419037199126063e-06\n",
      "remainder__x39: -3.419037199126063e-06\n",
      "remainder__x58: -3.419037199126063e-06\n",
      "encoder__x1_Gen Z: -6.838074398252126e-06\n",
      "encoder__x2_gain: -6.838074398252126e-06\n",
      "encoder__x5_near-hit: -6.838074398252126e-06\n",
      "remainder__x8: -6.838074398252126e-06\n",
      "remainder__x49: -6.838074398252126e-06\n",
      "remainder__x56: -1.025711159738929e-05\n",
      "remainder__x34: -1.3676148796515352e-05\n",
      "remainder__x0: -2.0514223194756375e-05\n",
      "encoder__x1_Millenials: -2.7352297593008502e-05\n",
      "remainder__x10: -3.077133479213456e-05\n",
      "remainder__x44: -3.077133479213456e-05\n",
      "remainder__x41: -3.4190371991260626e-05\n",
      "encoder__x5_loss: -3.760940919038669e-05\n",
      "remainder__x15: -4.1028446389523857e-05\n",
      "remainder__x9: -4.4447483588638816e-05\n",
      "remainder__x26: -4.4447483588638816e-05\n",
      "remainder__x17: -4.786652078777598e-05\n",
      "remainder__x18: -6.496170678339519e-05\n",
      "remainder__x32: -7.179978118163622e-05\n",
      "encoder__x2_draw: -7.179978118164732e-05\n",
      "encoder__x1_Gen X: -8.20568927790255e-05\n",
      "remainder__x20: -8.20568927790255e-05\n",
      "remainder__x36: -0.00011282822757113786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(classifier, X_train, y_train, n_repeats=10, random_state=42)\n",
    "\n",
    "# Get feature importances and feature names\n",
    "importances = result.importances_mean\n",
    "feature_names = ct.get_feature_names_out()\n",
    "\n",
    "# Sort feature importances\n",
    "feature_importance = list(zip(feature_names, importances))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print feature importances\n",
    "for feature, importance in feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10410     9]\n",
      " [ 1173     3]]\n",
      "Accuracy:  0.8980595084087969\n",
      "Precision:  0.5743654493654493\n",
      "Recall:  0.5008436069504105\n",
      "F1 Score:  0.47566405808838313\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 2min, named it datasets_2\n",
    "datasets_2 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '2min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_2['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_2['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10407    12]\n",
      " [ 1167     9]]\n",
      "Accuracy:  0.8983182406209573\n",
      "Precision:  0.6638709916314893\n",
      "Recall:  0.5032506596073499\n",
      "F1 Score:  0.48071481390695403\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 3min, named it datasets_3\n",
    "datasets_3 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '3min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_3['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_3['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10402    17]\n",
      " [ 1165    11]]\n",
      "Accuracy:  0.8980595084087969\n",
      "Precision:  0.6460697921426719\n",
      "Recall:  0.5038610534913649\n",
      "F1 Score:  0.4822554703340362\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 4min, named it datasets_4\n",
    "datasets_4 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '4min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_4['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_4['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10399    20]\n",
      " [ 1163    13]]\n",
      "Accuracy:  0.8979732643380768\n",
      "Precision:  0.646675630199242\n",
      "Recall:  0.5045674258761955\n",
      "F1 Score:  0.48384308442335094\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 5min, named it datasets_5\n",
    "datasets_5 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '5min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_5['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_5['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10399    20]\n",
      " [ 1163    13]]\n",
      "Accuracy:  0.8979732643380768\n",
      "Precision:  0.646675630199242\n",
      "Recall:  0.5045674258761955\n",
      "F1 Score:  0.48384308442335094\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 5min, named it datasets_5\n",
    "datasets_10 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '10min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_5['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_5['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10399    20]\n",
      " [ 1163    13]]\n",
      "Accuracy:  0.8979732643380768\n",
      "Precision:  0.646675630199242\n",
      "Recall:  0.5045674258761955\n",
      "F1 Score:  0.48384308442335094\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for 5min, named it datasets_5\n",
    "datasets_15 = cf.load_and_preprocess_datasets_min_ntop(months, data_path, '15min', filter)\n",
    "\n",
    "# Create training and test sets\n",
    "dt_train = pd.concat([datasets_5['dtf'+month[1:]] for month in months_train])\n",
    "dt_test = pd.concat([datasets_5['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "# # Seperate dependent and independent variables\n",
    "X_train = dt_train.iloc[:, :-1].values\n",
    "y_train = dt_train.iloc[:, -1].values\n",
    "X_test = dt_test.iloc[:, :-1].values\n",
    "y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "# Econde gender column (Binary)\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Binary Encode gender\n",
    "X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "\n",
    "# # Encode age_generartion, first_outoce, last_outcome columns\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 5])], remainder='passthrough')\n",
    "X_train = np.array(ct.fit_transform(X_train))\n",
    "X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "# # Feature Scaling\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale all columns except the encoded ones\n",
    "X_train[:, 14:] = sc.fit_transform(X_train[:, 14:])\n",
    "X_test[:, 14:] = sc.transform(X_test[:, 14:])\n",
    "\n",
    "classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('Precision: ', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remainder__x55: 0.0006222647702407436\n",
      "encoder__x1_Baby Boomers: 0.0003521608315098956\n",
      "remainder__x11: 0.00032480853391692044\n",
      "remainder__x23: 0.0002906181619256487\n",
      "remainder__x40: 0.00025984682713352524\n",
      "remainder__x18: 0.0002119803063458159\n",
      "remainder__x6: 0.0002017231947484377\n",
      "remainder__x20: 0.0002017231947484377\n",
      "remainder__x10: 0.00019830415754930054\n",
      "remainder__x19: 0.00018804704595193344\n",
      "remainder__x21: 0.00018804704595193344\n",
      "encoder__x1_Silent: 0.00016753282275716597\n",
      "remainder__x9: 0.00012992341356679037\n",
      "remainder__x51: 0.00012308533916852716\n",
      "remainder__x22: 0.00010599015317290794\n",
      "remainder__x45: 9.915207877465581e-05\n",
      "encoder__x1_Millenials: 9.573304157554086e-05\n",
      "remainder__x14: 6.154266958426912e-05\n",
      "remainder__x38: 5.812363238516527e-05\n",
      "encoder__x1_Gen X: 4.444748358868322e-05\n",
      "remainder__x33: 4.444748358868322e-05\n",
      "encoder__x1_Gen Z: 4.4447483588649915e-05\n",
      "remainder__x44: 4.4447483588638816e-05\n",
      "remainder__x28: 3.4190371991260626e-05\n",
      "remainder__x4: 2.7352297593019602e-05\n",
      "remainder__x26: 2.3933260393904643e-05\n",
      "remainder__x30: 2.3933260393882437e-05\n",
      "encoder__x2_gain: 1.7095185995641416e-05\n",
      "remainder__x7: 1.7095185995641416e-05\n",
      "remainder__x52: 1.367614879654866e-05\n",
      "remainder__x12: 6.838074398252126e-06\n",
      "remainder__x53: 4.4408920985006264e-17\n",
      "remainder__x42: 0.0\n",
      "remainder__x57: 0.0\n",
      "remainder__x27: -3.4190371991038583e-06\n",
      "remainder__x29: -3.4190371991149603e-06\n",
      "remainder__x41: -6.8380743982188186e-06\n",
      "remainder__x47: -1.025711159733378e-05\n",
      "encoder__x5_draw: -1.025711159734488e-05\n",
      "remainder__x8: -2.051422319467866e-05\n",
      "remainder__x39: -2.051422319468976e-05\n",
      "remainder__x36: -2.0514223194700864e-05\n",
      "encoder__x5_gain: -2.3933260393804723e-05\n",
      "remainder__x13: -2.3933260393815826e-05\n",
      "remainder__x0: -3.077133479206795e-05\n",
      "remainder__x16: -3.077133479206795e-05\n",
      "remainder__x25: -3.4190371991149604e-05\n",
      "remainder__x46: -3.4190371991149604e-05\n",
      "remainder__x59: -3.4190371991194016e-05\n",
      "encoder__x2_near-hit: -3.760940919027567e-05\n",
      "remainder__x48: -3.760940919029787e-05\n",
      "remainder__x58: -3.7609409190320074e-05\n",
      "encoder__x2_draw: -4.1028446389412834e-05\n",
      "remainder__x54: -5.128555798681322e-05\n",
      "remainder__x17: -5.470459518591708e-05\n",
      "remainder__x56: -5.812363238507645e-05\n",
      "remainder__x43: -6.496170678328416e-05\n",
      "remainder__x34: -7.521881838068456e-05\n",
      "remainder__x3: -8.547592997805164e-05\n",
      "remainder__x50: -8.547592997806274e-05\n",
      "encoder__x2_loss: -9.231400437630377e-05\n",
      "encoder__x5_near-hit: -9.573304157542984e-05\n",
      "remainder__x24: -0.00010257111597367086\n",
      "remainder__x35: -0.00011282822757104904\n",
      "encoder__x5_loss: -0.0001162472647701751\n",
      "remainder__x32: -0.00013334245076582763\n",
      "remainder__x49: -0.00014018052516405753\n",
      "remainder__x15: -0.00019488512035007455\n",
      "remainder__x37: -0.0002256564551421869\n",
      "remainder__x31: -0.00026668490153167743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(classifier, X_train, y_train, n_repeats=10, random_state=42)\n",
    "\n",
    "# Get feature importances and feature names\n",
    "importances = result.importances_mean\n",
    "feature_names = ct.get_feature_names_out()\n",
    "\n",
    "# Sort feature importances\n",
    "feature_importance = list(zip(feature_names, importances))\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print feature importances\n",
    "for feature, importance in feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "  Time &  Accuracy &  Precision &  Recall &  F1 Score \\\\\n",
      "\\midrule\n",
      " 1 min &     0.898 &      0.672 &   0.503 &     0.480 \\\\\n",
      " 5 min &     0.898 &      0.643 &   0.504 &     0.483 \\\\\n",
      "10 min &     0.898 &      0.694 &   0.515 &     0.504 \\\\\n",
      "15 min &     0.900 &      0.751 &   0.525 &     0.524 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vp/1skwx2kd29s4fxnxx7tt9r6w0000gn/T/ipykernel_95362/2521130281.py:67: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = results_df.to_latex(index=False, escape=False)\n"
     ]
    }
   ],
   "source": [
    "# Table creation \n",
    "# Define time intervals\n",
    "time_intervals = [1, 5, 10, 15]\n",
    "\n",
    "# Initialize lists to store results\n",
    "results = []\n",
    "for time_interval in time_intervals:\n",
    "    # Load dataset for the specific time interval\n",
    "    file_name = f'{time_interval}min'\n",
    "    # Load the datasets\n",
    "    datasets = cf.load_and_preprocess_datasets_min_ntop(months, data_path, file_name, filter)\n",
    "\n",
    "    # Create training and test sets\n",
    "    dt_train = pd.concat([datasets['dtf'+month[1:]] for month in months_train])\n",
    "    dt_test = pd.concat([datasets['dtf'+month[1:]] for month in months_test])\n",
    "\n",
    "    # # Seperate dependent and independent variables\n",
    "    X_train = dt_train.iloc[:, :-1].values\n",
    "    y_train = dt_train.iloc[:, -1].values\n",
    "    X_test = dt_test.iloc[:, :-1].values\n",
    "    y_test = dt_test.iloc[:, -1].values\n",
    "\n",
    "    # Econde gender column (Binary)\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Binary Encode gender\n",
    "    X_train[:, 0] = le.fit_transform(X_train[:, 0])\n",
    "    X_test[:, 0] = le.fit_transform(X_test[:, 0])\n",
    "\n",
    "    # # # Encode age_generartion, first_outoce, last_outcome, time of day columns\n",
    "    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1, 2, 3, 4, 7])], remainder='passthrough')\n",
    "    X_train = np.array(ct.fit_transform(X_train))\n",
    "    X_test = np.array(ct.transform(X_test))\n",
    "\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    # Scale all columns except the encoded ones\n",
    "    X_train[:, 25:] = sc.fit_transform(X_train[:, 25:])\n",
    "    X_test[:, 25:] = sc.transform(X_test[:, 25:])\n",
    "\n",
    "    classifier = LogisticRegression(random_state = 0, max_iter=1000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Append results for this time interval\n",
    "    results.append([f'{time_interval} min', round(accuracy, 3), round(precision, 3), round(recall, 3), round(f1, 3)])\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "columns = ['Time', 'Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Print the results as a table\n",
    "# print(results_df)\n",
    "\n",
    "# Print the results as a LaTeX table\n",
    "latex_table = results_df.to_latex(index=False, escape=False)\n",
    "print(latex_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
